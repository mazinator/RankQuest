{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8usSW9Bwv4h"
   },
   "source": [
    "# AIR - Exercise in Google Colab\n",
    "\n",
    "## Colab Preparation\n",
    "\n",
    "Open via google drive -> right click: open with Colab\n",
    "\n",
    "**Get a GPU**\n",
    "\n",
    "Toolbar -> Runtime -> Change Runtime Type -> GPU\n",
    "\n",
    "**Mount Google Drive**\n",
    "\n",
    "* Download data and clone your github repo to your Google Drive folder\n",
    "* Use Google Drive as connection between Github and Colab (Could also use direct github access, but re-submitting credentials might be annoying)\n",
    "* Commit to Github locally from the synced drive\n",
    "\n",
    "**Keep Alive**\n",
    "\n",
    "When training google colab tends to kick you out, This might help: https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n",
    "\n",
    "**Get Started**\n",
    "\n",
    "Run the following script to mount google drive and install needed python packages. Pytorch comes pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sfiw_6jZ0uWa"
   },
   "outputs": [],
   "source": [
    "#%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUVVDw1m2sed"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Version:\",torch.__version__)\n",
    "print(\"Has GPU:\",torch.cuda.is_available()) # check that 1 gpu is available\n",
    "print(\"Random tensor:\",torch.rand(10)) # check that pytorch works "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if downloading data is desired\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "DATA_PATH.mkdir(exist_ok=True, parents=True)\n",
    "DATA_ZIP = DATA_PATH / \"data.zip\"\n",
    "DATA_URL = \"https://owncloud.tuwien.ac.at/index.php/s/QA4LEtxdBokqdNx/download\"\n",
    "\n",
    "GLOVE_ZIP = DATA_PATH / \"glove.42B.300d.zip\"\n",
    "GLOVE_URL = \"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "\n",
    "\n",
    "if not DATA_ZIP.exists():\n",
    "    r = requests.get(DATA_URL)\n",
    "    DATA_ZIP.write_bytes(r.content)\n",
    "    del r\n",
    "\n",
    "    with zipfile.ZipFile(DATA_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)\n",
    "\n",
    "    for f in DATA_PATH.rglob(\"air-exercise-2/*/*\"):\n",
    "        f.rename(DATA_PATH / f.name)\n",
    "\n",
    "    for f in DATA_PATH.rglob(\"air-exercise-2/*\"):\n",
    "        if f.is_dir():\n",
    "            f.rmdir()\n",
    "\n",
    "    (DATA_PATH / \"air-exercise-2\").rmdir()\n",
    "\n",
    "\n",
    "if not GLOVE_ZIP.exists():\n",
    "    r = requests.get(GLOVE_URL)\n",
    "    GLOVE_ZIP.write_bytes(r.content)\n",
    "    del r\n",
    "\n",
    "    with zipfile.ZipFile(GLOVE_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvQMmxs0x_x8"
   },
   "source": [
    "# Re Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET CHECKPOINT PATH\n",
    "#TODO checkpoint folder \n",
    "CHECKPOINT_FOLDER_PATH = \"../checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check epochs\n",
    "MAX_EPOCHS = 10\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import Params, Tqdm\n",
    "from allennlp.common.util import prepare_environment\n",
    "from allennlp.data.dataloader import PyTorchDataLoader\n",
    "\n",
    "params = Params({'random_seed': 42, 'numpy_seed': 42, 'pytorch_seed': 42})\n",
    "prepare_environment(params)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "from data_loading import *\n",
    "from model_knrm import *\n",
    "from model_tk import *\n",
    "from core_metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change paths to your data directory\n",
    "config = {\n",
    "    \"vocab_directory\": \"../data/allen_vocab_lower_10\",\n",
    "    \"pre_trained_embedding\": \"../data/glove.42B.300d.txt\",\n",
    "    \"train_data\": \"../data/triples.train.tsv\",\n",
    "    \"validation_data\": \"../data/msmarco_tuples.validation.tsv\",\n",
    "    \"test_data\":\"../data/msmarco_tuples.test.tsv\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"qrels_file\": \"../data/msmarco_qrels.txt\",\n",
    "    \"fira_tuples\": \"../data/fira-22.tuples.tsv\",\n",
    "    \"fira_qrels_baseline\": \"../data/fira-22.baseline-qrels.tsv\",\n",
    "    \"fira_qrels_part1\": \"../out/fira-22.qrels.tsv\",\n",
    "    \"checkpoint_dir\": CHECKPOINT_FOLDER_PATH\n",
    "}\n",
    "\n",
    "vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n",
    "tokens_embedder = Embedding(vocab=vocab,\n",
    "                           pretrained_file= config[\"pre_trained_embedding\"],\n",
    "                           embedding_dim=300,\n",
    "                           trainable=True,\n",
    "                           padding_index=0)\n",
    "word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(model_used):\n",
    "    checkpoint_files = [f for f in os.listdir(config[\"checkpoint_dir\"]) if f.startswith('checkpoint')]\n",
    "    if not checkpoint_files:\n",
    "        return None, 0\n",
    "\n",
    "    epochs = [int(re.search(r'epoch_(\\d+)', f).group(1)) for f in checkpoint_files]\n",
    "    latest_epoch = max(epochs)\n",
    "    latest_checkpoint = f\"checkpoint_{model_used}_epoch_{latest_epoch}.pt\"\n",
    "\n",
    "    return os.path.join(config[\"checkpoint_dir\"], latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_IEUP_2-099",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model_used, checkpoint = False, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "\n",
    "    ##### TRIPLES LOADER ######\n",
    "    _triple_reader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "    _triple_reader = _triple_reader.read(config[\"train_data\"])\n",
    "    _triple_reader.index_with(vocab)\n",
    "    triple_loader = PyTorchDataLoader(_triple_reader, batch_size=32)\n",
    "\n",
    "    ##### TUPLES VALIDATION MSMARCO LOADER ######\n",
    "    _tuple_reader_validation = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "    _tuple_reader_validation = _tuple_reader_validation.read(config[\"validation_data\"])\n",
    "    _tuple_reader_validation.index_with(vocab)\n",
    "    validation_loader = PyTorchDataLoader(_tuple_reader_validation, batch_size=128)\n",
    "\n",
    "\n",
    "    output_file = f\"../out/test_metrics_msmarco_{model_used}.txt\"\n",
    "\n",
    "    if model_used == \"knrm\":\n",
    "            model = KNRM(word_embedder, n_kernels=11)\n",
    "    elif model_used == \"tk\":\n",
    "            model = TK(word_embedder, n_kernels=11, n_layers = 2, n_tf_dim = 300, n_tf_heads = 10)\n",
    "    else:\n",
    "        raise Exception(\"Model type not supported\")\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"]) # Adam optimizer\n",
    "\n",
    "    if(checkpoint):\n",
    "        checkpoint_path = find_latest_checkpoint(model_used)\n",
    "        print(checkpoint_path)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        #loss = checkpoint['loss']\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    qrels = load_qrels(config[\"qrels_file\"])\n",
    "    loss_function = nn.MarginRankingLoss(margin=1.0) # Marking Ranking loss as loss function\n",
    "\n",
    "    print('Model',model,'total parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    print('Network:', model)\n",
    "\n",
    "\n",
    "    batch_counter = 0\n",
    "\n",
    "    max_epoch = MAX_EPOCHS\n",
    "    epochs_trained = 0\n",
    "\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_counter = 0\n",
    "        epochs_trained += 1\n",
    "\n",
    "        for batch in Tqdm.tqdm(triple_loader):\n",
    "            #if(batch_counter > 2):\n",
    "            #    break\n",
    "            batch = {k1: {k2: {k3: v3.to(device) for k3, v3 in v2.items()} for k2, v2 in v1.items()} for k1, v1 in batch.items()}\n",
    "            batch_counter += 1\n",
    "            optimizer.zero_grad()\n",
    "            positive = model.forward(batch['query_tokens'], batch['doc_pos_tokens'])\n",
    "            negative = model.forward(batch['query_tokens'], batch['doc_neg_tokens'])\n",
    "            target = torch.ones(positive.size()).to(device)  # target tensor with ones, since positive should be ranked higher than negative\n",
    "            loss = loss_function(positive, negative, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / batch_counter}\")\n",
    "\n",
    "        # checkpoint\n",
    "        checkpoint_path = os.path.join(config[\"checkpoint_dir\"], f\"checkpoint_{model_used}_epoch_{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #'loss': total_loss / batch_counter,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()\n",
    "        rankings = {}\n",
    "        total_validation_loss = 0.0\n",
    "        validation_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in Tqdm.tqdm(validation_loader):\n",
    "                batch.update({k1: {k2: {k3: v3.to(device) for k3, v3 in v2.items()} for k2, v2 in v1.items()} for k1, v1 in batch.items() if k1 in [\"query_tokens\", \"doc_tokens\"]})\n",
    "                outputs = model.forward(batch['query_tokens'], batch['doc_tokens'])\n",
    "\n",
    "                target = torch.ones(outputs.size()).to(device)\n",
    "                loss = loss_function(outputs, target, target)\n",
    "                total_validation_loss += loss.item()\n",
    "                validation_batches += 1\n",
    "\n",
    "                for i, query_id in enumerate(batch['query_id']):\n",
    "                    if query_id not in rankings:\n",
    "                        rankings[query_id] = []\n",
    "                    rankings[query_id].append((batch['doc_id'][i], outputs[i].item()))\n",
    "\n",
    "        #calculate validation loss\n",
    "        average_validation_loss = total_validation_loss / validation_batches\n",
    "        validation_losses.append(average_validation_loss)\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss: {average_validation_loss}\")\n",
    "\n",
    "        # Convert unrolled results to ranked results\n",
    "        ranked_results = unrolled_to_ranked_result(rankings)\n",
    "\n",
    "        # Calculate and print validation metrics\n",
    "        validation_metrics = calculate_metrics_plain(ranked_results, qrels, binarization_point=1)\n",
    "        print('Validation Metrics:')\n",
    "        for metric in validation_metrics:\n",
    "            print('{}: {}'.format(metric, validation_metrics[metric]))\n",
    "\n",
    "        # # Early stopping check\n",
    "        # TODO CHECK if early stopping desired\n",
    "        # if len(validation_losses) > 2:\n",
    "        #     if validation_losses[-1] > sum(validation_losses[-3:-1]) / 2:\n",
    "        #         print(f\"Stopping early at epoch {epoch + 1}, validation loss increases over 2 epochs average.\")\n",
    "        #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO INCLUDE IF TRAINING DESIRED\n",
    "# checkpoint=True if training should be started from latest available checkpoint\n",
    "\n",
    "#train_model(\"knrm\", checkpoint=False) \n",
    "#train_model(\"tk\", checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model with lowest validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_loss(model_path, model_used, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    #load model\n",
    "    if model_used == \"knrm\":\n",
    "            model = KNRM(word_embedder, n_kernels=11)\n",
    "    elif model_used == \"tk\":\n",
    "            model = TK(word_embedder, n_kernels=11, n_layers = 2, n_tf_dim = 300, n_tf_heads = 10)\n",
    "    else:\n",
    "        raise Exception(\"Model type not supported\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    #check if model exists\n",
    "    \n",
    "    try:\n",
    "        model_checkpoint = torch.load(model_path, map_location=device)\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Path to model does not exist: {model_path}\")\n",
    "         return 10000\n",
    "    \n",
    "    model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "\n",
    "    # set validation loader\n",
    "    _tuple_reader_validation = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "    _tuple_reader_validation = _tuple_reader_validation.read(config[\"validation_data\"])\n",
    "    _tuple_reader_validation.index_with(vocab)\n",
    "    validation_loader = PyTorchDataLoader(_tuple_reader_validation, batch_size=128)\n",
    "    \n",
    "    model.eval()\n",
    "    loss_function = nn.MarginRankingLoss(margin=1.0)\n",
    "    rankings = {}\n",
    "    total_validation_loss = 0.0\n",
    "    validation_batches = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch in Tqdm.tqdm(validation_loader):\n",
    "            batch.update({k1: {k2: {k3: v3.to(device) for k3, v3 in v2.items()} for k2, v2 in v1.items()} for k1, v1 in batch.items() if k1 in [\"query_tokens\", \"doc_tokens\"]})\n",
    "            outputs = model.forward(batch['query_tokens'], batch['doc_tokens'])\n",
    "\n",
    "            target = torch.ones(outputs.size()).to(device)\n",
    "            loss = loss_function(outputs, target, target)\n",
    "            total_validation_loss += loss.item()\n",
    "            validation_batches += 1\n",
    "\n",
    "            for i, query_id in enumerate(batch['query_id']):\n",
    "                if query_id not in rankings:\n",
    "                    rankings[query_id] = []\n",
    "                rankings[query_id].append((batch['doc_id'][i], outputs[i].item()))\n",
    "\n",
    "    #validation loss\n",
    "    return total_validation_loss / validation_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns model with lowest validation loss\n",
    "def get_best_model(model_type):\n",
    "    best_validation_loss = 10000\n",
    "    best_model_path = None\n",
    "    best_epoch = None\n",
    "\n",
    "    for i in range (1, MAX_EPOCHS+1):\n",
    "        current_checkpoint_path = os.path.join(CHECKPOINT_FOLDER_PATH, f\"checkpoint_{model_type}_epoch_{i}.pt\")\n",
    "        print(f\"Loading epoch {i} of {model_type} model, searching for path {current_checkpoint_path}\")\n",
    "        current_validation_loss = calculate_validation_loss(current_checkpoint_path, model_type)\n",
    "        print(f\"validation loss in epoch {i}: {current_validation_loss}\")\n",
    "\n",
    "        if(current_validation_loss < best_validation_loss):\n",
    "            best_model_path = current_checkpoint_path\n",
    "            best_validation_loss = current_validation_loss\n",
    "            best_epoch = i\n",
    "\n",
    "\n",
    "    print(f\"model with lowest validation loss of {model_type} was in epoch {best_epoch}: {best_model_path}\")\n",
    "\n",
    "    return best_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model = model with lowest validation loss\n",
    "best_model_path_knrm = get_best_model(\"knrm\")\n",
    "best_model_path_tk = get_best_model(\"tk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tk is better, load best tk model\n",
    "best_model = TK(word_embedder, n_kernels=11, n_layers = 2, n_tf_dim = 300, n_tf_heads = 10)\n",
    "model_checkpoint = torch.load(best_model_path_tk, map_location=device)\n",
    "\n",
    "best_model.load_state_dict(model_checkpoint['model_state_dict'])\n",
    "best_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_msmarco(model, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "\n",
    "    _tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "    _tuple_reader = _tuple_reader.read(config[\"test_data\"])\n",
    "    _tuple_reader.index_with(vocab)\n",
    "    test_loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n",
    "\n",
    "    model.eval()\n",
    "    rankings = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in Tqdm.tqdm(test_loader):\n",
    "            batch.update({k1: {k2: {k3: v3.to(device) for k3, v3 in v2.items()} for k2, v2 in v1.items()} for k1, v1 in batch.items() if k1 in [\"query_tokens\", \"doc_tokens\"]})\n",
    "            outputs = model.forward(batch['query_tokens'], batch['doc_tokens'])\n",
    "            for i, query_id in enumerate(batch['query_id']):\n",
    "                if query_id not in rankings:\n",
    "                    rankings[query_id] = []\n",
    "                rankings[query_id].append((batch['doc_id'][i], outputs[i].item()))\n",
    "\n",
    "    # Convert unrolled results to ranked results\n",
    "    ranked_results = unrolled_to_ranked_result(rankings)\n",
    "\n",
    "    qrels = load_qrels(config[\"qrels_file\"])\n",
    "\n",
    "    # Calculate and print test metrics\n",
    "    test_metrics = calculate_metrics_plain(ranked_results, qrels, binarization_point=1)\n",
    "    print('#####################')\n",
    "    print(\"metrics on msmacro testset\")\n",
    "    for metric in test_metrics:\n",
    "        print('{}: {}'.format(metric, test_metrics[metric]))\n",
    "    print('#####################')\n",
    "\n",
    "    # Store test metrics in a txt file\n",
    "    output_file = \"../out/test_metrics_msmarco_tk_best.txt\"\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write('#####################\\n')\n",
    "        f.write(f\"Dataset: msmarco\\n\")\n",
    "        f.write(f\"Model used: {model}\\n\")\n",
    "        f.write('#####################\\n')\n",
    "        for metric in test_metrics:\n",
    "            f.write('{}: {}\\n'.format(metric, test_metrics[metric]))\n",
    "        f.write('#####################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_msmarco(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FiRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_fira(model, qrels_path, judgement_type, chosen_binarization_point, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    ##### TUPLES FIRA LOADER ######\n",
    "    _tuple_reader_fira = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "    _tuple_reader_fira = _tuple_reader_fira.read(config[\"fira_tuples\"])\n",
    "    _tuple_reader_fira.index_with(vocab)\n",
    "    fira_loader = PyTorchDataLoader(_tuple_reader_fira, batch_size=128)\n",
    "\n",
    "\n",
    "    #### calculate for baseline\n",
    "    baseline_qrels = load_qrels(qrels_path)\n",
    "    config[\"output_file_fira_baseline\"] = f\"../out/test_metrics_fira_{judgement_type}_binarizationpoint_{chosen_binarization_point}.txt\"\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    rankings = {}\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in Tqdm.tqdm(fira_loader):\n",
    "            batch.update({k1: {k2: {k3: v3.to(device) for k3, v3 in v2.items()} for k2, v2 in v1.items()} for k1, v1 in batch.items() if k1 in [\"query_tokens\", \"doc_tokens\"]})\n",
    "            outputs = model.forward(batch['query_tokens'], batch['doc_tokens'])\n",
    "            for i, query_id in enumerate(batch['query_id']):\n",
    "                if query_id not in rankings:\n",
    "                    rankings[query_id] = []\n",
    "                rankings[query_id].append((batch['doc_id'][i], outputs[i].item()))\n",
    "                predictions.append((query_id, batch['doc_id'][i], outputs[i].item()))\n",
    "\n",
    "    # Convert unrolled results to ranked results\n",
    "    ranked_results = unrolled_to_ranked_result(rankings)\n",
    "\n",
    "    # Calculate and print FiRA test metrics with baseline qrels\n",
    "    fira_test_metrics = calculate_metrics_plain(ranked_results, baseline_qrels, binarization_point=chosen_binarization_point)\n",
    "    print('#####################')\n",
    "    print(f'FiRA Test Metrics with Judgement {judgement_type}:')\n",
    "    for metric in fira_test_metrics:\n",
    "        print('{}: {}'.format(metric, fira_test_metrics[metric]))\n",
    "    print('#####################')\n",
    "\n",
    "    # Store predictions in a txt file\n",
    "    predictions_file = f\"../out/fira_predictions_{judgement_type}_tk_binarizationpoint_{chosen_binarization_point}.txt\"\n",
    "    with open(predictions_file, \"w\", encoding='utf-8') as f:\n",
    "        for query_id, doc_id, score in predictions:\n",
    "            f.write(f\"{query_id}\\t{doc_id}\\t{score}\\n\")\n",
    "\n",
    "    output_file_fira = f\"../out/fira_metrics_tk_judgementtype_{judgement_type}_binarizationpoint_{chosen_binarization_point}.txt\"\n",
    "\n",
    "    # Store evaluation metric of FiRA Baseline in a txt file\n",
    "    with open(output_file_fira, \"w\", encoding='utf-8') as f:\n",
    "        f.write('#####################\\n')\n",
    "        f.write(f\"Dataset: FiRA\\n\")\n",
    "        f.write(f\"Model used: tk\\n\")\n",
    "        f.write(f\"binarization point: {chosen_binarization_point}\\n\")\n",
    "        f.write('#####################\\n')\n",
    "        f.write(f'FiRA Test Metrics with Judgement {judgement_type}:\\n')\n",
    "        for metric in fira_test_metrics:\n",
    "            f.write('{}: {}\\n'.format(metric, fira_test_metrics[metric]))\n",
    "        f.write('#####################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test fira with both judgements with binarization points 1 and 2\n",
    "test_model_fira(best_model, config['fira_qrels_baseline'], 'baseline', 1)\n",
    "test_model_fira(best_model, config['fira_qrels_baseline'], 'baseline', 2)\n",
    "test_model_fira(best_model, config['fira_qrels_part1'], 'part1', 1)\n",
    "test_model_fira(best_model, config['fira_qrels_part1'], 'part1', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create passage results needed for part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model #set best tk model\n",
    "\n",
    "msmarco_fira_qa_path = \"../data/msmarco-fira-21.qrels.qa-answers.tsv\"\n",
    "\n",
    "_tuple_reader_fira = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_tuple_reader_fira = _tuple_reader_fira.read(msmarco_fira_qa_path)\n",
    "_tuple_reader_fira.index_with(vocab)\n",
    "fira_loader = PyTorchDataLoader(_tuple_reader_fira, batch_size=128)\n",
    "\n",
    "_tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_tuple_reader = _tuple_reader.read(config[\"test_data\"])\n",
    "_tuple_reader.index_with(vocab)\n",
    "test_loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n",
    "\n",
    "model.eval()\n",
    "rankings = {}\n",
    "with torch.no_grad():\n",
    "    for batch in Tqdm.tqdm(test_loader):\n",
    "        batch.update({k1: {k2: {k3: v3.to(device) for k3, v3 in v2.items()} for k2, v2 in v1.items()} for k1, v1 in batch.items() if k1 in [\"query_tokens\", \"doc_tokens\"]})\n",
    "        outputs = model.forward(batch['query_tokens'], batch['doc_tokens'])\n",
    "        for i, query_id in enumerate(batch['query_id']):\n",
    "            if query_id not in rankings:\n",
    "                rankings[query_id] = []\n",
    "            rankings[query_id].append((batch['doc_id'][i], outputs[i].item()))\n",
    "\n",
    "# Convert unrolled results to ranked results\n",
    "ranked_results = unrolled_to_ranked_result(rankings)\n",
    "qrels = load_qrels(config[\"qrels_file\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save ranked results\n",
    "\n",
    "output_file_predictions = \"../out/msmarco_predictions_qa-answer-top1.txt\"\n",
    "\n",
    "if os.path.exists(output_file_predictions):\n",
    "    os.remove(output_file_predictions)\n",
    "\n",
    "with open(output_file_predictions, 'w') as f:\n",
    "    f.write(\"query_id\\tdoc_id\\n\")\n",
    "    for key, value in ranked_results.items():\n",
    "        if value:\n",
    "            f.write(f\"{key}\\t{value[0]}\\n\") #only keep first doc_id (top 1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
